## Формулировка задачи
Большинство веб-страниц сейчас перегружено всевозможной рекламой... Наша задача «вытащить»
из веб-страницы только полезную информацию, отбросив весь «мусор» (навигацию, рекламу и тд).

Полученный текст нужно отформатировать для максимально комфортного чтения в любом
текстовом редакторе. Правила форматирования: 
* ширина строки не больше 80 символов (еслибольше, переносим по словам), абзацы и заголовки отбиваются пустой строкой. 
* Если в тексте встречаются ссылки, то URL вставить в текст в квадратных скобках. Остальные правила на ваше
усмотрение.

Программа оформляется в виде утилиты командной строки, которой в качестве параметра
указывается произвольный URL. Она извлекает по этому URL страницу, обрабатывает ее и
формирует текстовый файл с текстом статьи, представленной на данной странице.

В качестве примера можно взять любую статью на lenta.ru, gazeta.ru и тд

Алгоритм должен быть максимально универсальным, то есть работать на большинстве сайтов.

__Задача 1*__: Имя выходного файла должно формироваться автоматически по URL.
Примерно так:
http://lenta.ru/news/2013/03/dtp/index.html => [CUR_DIR]/lenta.ru/news/2013/03/dtp/index.txt

__Задача 2*__: Программа должна поддаваться настройке – в отдельном файле/файлах
задаются шаблоны обработки страниц.

## Требования к выполнению задачи
1. Задача выполняется на Python с использованием классов. Не должно использоваться
сторонних библиотек, впрямую решающих задачу.
2. Предпочтительная среда выполнения – MS Windows.
3. Решение должно состоять из документа, описывающего алгоритм, исходных кодов
программы, исполняемого модуля.
4. Приложите список URL, на которых вы проверяли свое решение. И результаты проверки.
5. Желательно указать направление дальнейшего улучшения/развития программы.

## Сторонние библиотеки, используемые в ходе работы:
- BeautifulSoup
- Requests
* Также использован модуль "OS" для работы с директориями ОС

## Идея работы:
* За основные контейнеры обработки текста были приняты теги "p" и "a" (для ссылок).
- Страница скачивается с помощью библиотеки Requests.
- Формируется по контенту страницы объект BeautifulSoup.
- С помощью soup и настроек обработки файл очищается от мусора (ненужных тегов и пробелов).
- Очищенный текст форматируется согласно настройкам обрабоки файла. 
- Обработанный и форматированный текст сохраняется в документе в определённых директориях с помощью модуля OS.
- Работа сопровождается интерфейсом (общением с пользователем).

## Настройки обработки:
1. text_tags_list = ["p", "a", "span", "b", "i", "u"] - список тегов, из которых будет извлекаться текст.
2. main_text_tag = ["p"] - основной тег для извлечения текста.
3. main_link_tag = ["a"] - основной тег для извлечения ссылки.
4. head_tag_list = ["h1", "h2", "h3", "h4"] - список тегов, из которых будут извлекаться заголовки.
5. strings_characters = 80 - количество символов в строке (по умолчанию не превышает 80 символов).

## Описание файлов:
- main - основной исполняемый файл
- command_reference.py - справочник команд (обработка команд)
- download_article.py - класс Website, отвечающий за скачивание html-статьи
- html_articles - место сохранения html-статей
- converting_tags - класс TagConverter, отвечающий за преобразование тегов html-статьи
- txt_articles - место сохранения текстовых статей (РЕЗУЛЬТАТЫ ПРОВЕРКИ URL-АДРЕСОВ)

## Проверенные URL (результаты проверки находятся в директории txt_articles):
1. https://habr.com/ru/news/t/653373/
2. https://habr.com/ru/post/653605/
3. https://www.gazeta.ru/style/2022/02/26/14575933.shtml
4. https://lenta.ru/news/2022/02/27/durov/
5. https://moslenta.ru/news/city/ipotechnye-kredity-28-02-2022.htm
6. https://zen.yandex.ru/media/moya_skotinka/kak-uhajivat-za-krysoi-doma-kuda-poselit-i-kak-kormit-5dd1562eaa9fe536eeed64d5

## Дальнейшее улучшение/развитие программы:
- Изменить логику переноса для длинных ссылок.
- Изменить форматирование текста (например, центрировать его и добавить отступ первой строки абзаца).
- Организовать чтение списочных элементов, без чтения меню сайта.
- Реализовать в отдельном файле/файлах использование шаблонов обработки страниц.
